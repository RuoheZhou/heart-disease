#!/usr/bin/env python
# coding: utf-8

# In[2]:


import pandas as pd
import numpy as np
from sklearn import preprocessing
import matplotlib.pyplot as plt 

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
import seaborn as sns


# In[13]:


heart = pd.read_csv("/Users/ruohezhou/Documents/hackathon/heart.csv")
heart.head()


# In[14]:


heart.columns


# In[18]:


le = preprocessing.LabelEncoder()
for i in heart.columns:
    encode = le.fit_transform(heart[i])
    heart[i] = encode
heart.head()


# In[19]:


heart.shape


# In[36]:


heart['Age']


# In[42]:


x = np.array(heart[heart.columns[:11]])
y = np.array(heart.loc[:, 'HeartDisease'])


# In[58]:


from sklearn.model_selection import train_test_split

X_temp, X_test, y_temp, y_test =         train_test_split(x, y, test_size=0.3, 
                         shuffle=True, random_state=1, stratify=y)

X_train, X_valid, y_train, y_valid =         train_test_split(X_temp, y_temp, test_size=0.3,
                         shuffle=True, random_state=1, stratify=y_temp)


# In[59]:


#standardization
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler().fit(X_train)

X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

print('Train size', X_train.shape, 'class proportions', np.bincount(y_train))
print('Valid size', X_valid.shape, 'class proportions', np.bincount(y_valid))
print('Test size', X_test.shape, 'class proportions', np.bincount(y_test))


# ##Decision Tree

# In[73]:


from sklearn.tree import DecisionTreeClassifier


tree = DecisionTreeClassifier(random_state=1, min_samples_leaf=15)
tree.fit(X_train, y_train)

train_acc = tree.score(X_train, y_train)
valid_acc = tree.score(X_valid, y_valid)
test_acc = tree.score(X_test, y_test)

print(f'Training accuracy: {train_acc*100:.2f}%')
print(f'Validation accuracy: {valid_acc*100:.2f}%')
print(f'Depth: {tree.get_depth()}')


# In[61]:


heart.columns


# In[62]:


get_ipython().run_line_magic('matplotlib', 'inline')
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree


plt.figure(figsize=(5, 7))

plot_tree(tree, 
          filled=True, 
          rounded=True,
          feature_names=['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol', 'FastingBS',
       'RestingECG', 'MaxHR', 'ExerciseAngina', 'Oldpeak', 'ST_Slope'],
          class_names=['no heart disease', 
                       'heart disease']
         )

#plt.tight_layout()
plt.show()


# In[66]:


corrMatrix = heart.corr()
import seaborn as sn
import matplotlib.pyplot as plt
plt.figure(figsize=(16,5))
sn.heatmap(corrMatrix, annot=True)
plt.show()


# In[71]:


all_train_acc = []
all_valid_acc = []
all_hyperparam = []


for i in range(1, 40):
    tree = DecisionTreeClassifier(min_samples_leaf=i, random_state=1)
    tree.fit(X_train, y_train)
    train_acc = tree.score(X_train, y_train)
    valid_acc = tree.score(X_valid, y_valid)
    
    all_train_acc.append(train_acc*100)
    all_valid_acc.append(valid_acc*100)
    all_hyperparam.append(i)
    
plt.plot(all_hyperparam, all_train_acc, label='Training accuracy')
plt.plot(all_hyperparam, all_valid_acc, ls='--', label='Validation accuracy')
plt.xlabel('min_samples_leaf')
plt.ylabel('Accuracy in %')
plt.legend()
plt.show()


# In[74]:


tree.predict(X_test)


# KNN classifier

# In[79]:


from sklearn.neighbors import KNeighborsClassifier
clf = KNeighborsClassifier().fit(X_train, y_train)
clf.score(X_valid, y_valid)

clf.score(X_train, y_train)


# In[80]:


clf.score(X_test, y_test)


# In[81]:


clf.predict(X_test)


# In[87]:


from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression()

rfe = RFE(logreg, 12)

rfe = rfe.fit(X_train, y_train)


# In[ ]:





# In[88]:


rfe.score(X_test, y_test)


# In[89]:


rfe.score(X_valid, y_valid)


# In[93]:


from sklearn.model_selection import GridSearchCV
from mlxtend.evaluate import PredefinedHoldoutSplit
from sklearn.pipeline import make_pipeline


train_ind, valid_ind = train_test_split(np.arange(X_train.shape[0]),
                                        test_size=0.2, shuffle=True,
                                        random_state=0, stratify=y_train)

pipe = make_pipeline(StandardScaler(),
                     KNeighborsClassifier())

params = {'kneighborsclassifier__n_neighbors': [1, 3, 5, 7],
          'kneighborsclassifier__p': [1, 2]}

split = PredefinedHoldoutSplit(valid_indices=valid_ind)

grid = GridSearchCV(pipe,
                    param_grid=params,
                    cv=split)

grid.fit(X_train, y_train)


# In[94]:


grid.cv_results_


# In[95]:


for i,j in zip(grid.cv_results_['params'], grid.cv_results_['mean_test_score']):
    print(i, j)


# In[96]:


print(grid.best_score_)
print(grid.best_params_)


# In[97]:


clf = grid.best_estimator_

print('Test accuracy: %.2f%%' % (clf.score(X_test, y_test)*100))


# In[116]:


from keras.models import Sequential
from keras.layers import Dense
from keras.callbacks import EarlyStopping


# In[143]:


model = Sequential()
model.add(Dense(16, input_shape=(x.shape[1],), activation='relu')) # Add an input shape! (features,)
model.add(Dense(16, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.add(Dense(1, activation = 'tanh'))
model.summary() 


model.compile(optimizer='Adam', 
              loss='binary_crossentropy',
              metrics=['accuracy'])

  
es = EarlyStopping(monitor='val_accuracy', 
                                   mode='max', 
                                   patience=10,
                                   restore_best_weights=True)


history = model.fit(X_train,
                    y_train,
                    callbacks=[es],
                    epochs=80, 
                    batch_size=10,
                    validation_split=0.2,
                    shuffle=True,
                    verbose=1)


# In[171]:


history_dict = history.history

loss_values = history_dict['loss'] 
val_loss_values = history_dict['val_loss'] 


epochs = range(1, len(loss_values) + 1) 

# plot
plt.plot(epochs, loss_values, 'bo', label='Training loss')
plt.plot(epochs, val_loss_values, 'grey', label='Validation loss')
plt.title('Training loss & validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()


# In[170]:


acc = history.history['accuracy']
val_acc = history.history['val_accuracy']


epochs = range(1, len(acc) + 1)


plt.plot(epochs, acc, 'bo', label='Training accuracy')

plt.plot(epochs, val_acc, 'grey', label='Validation accuracy')
plt.title('Training accuracy & validation accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

np.max(val_acc)


# In[146]:


from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report


model.predict(X_test) 
np.round(model.predict(X_test),0) 


preds = np.round(model.predict(X_test),0)


print(confusion_matrix(y_test, preds)) 


print(classification_report(y_test, preds))


# In[147]:


model.predict(X_test)


# In[148]:


np.round(model.predict(X_test),0)


# In[ ]:


random forest


# In[164]:


from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators=250,
                                random_state=0)

forest.fit(X_train, y_train)
forest.feature_importances_


# 0:'Age'
# 1:'Sex'
# 2:'ChestPainType'
# 3:'RestingBP'
# 4:'Cholesterol'
# 5:'FastingBS'
# 6:'RestingECG'
# 7:'MaxHR'
# 8:'ExerciseAngina'
# 9:'Oldpeak'
# 10:'ST_Slope'

# In[165]:


std = np.std([tree.feature_importances_ for tree in forest.estimators_],
             axis=0)
indices = np.argsort(forest.feature_importances_)[::-1]

# Plot the feature importances of the forest
plt.figure()
plt.title("feature importance(random forest)")
plt.bar(range(x.shape[1]), importance_vals[indices],
        yerr=std[indices], align="center")
plt.xticks(range(x.shape[1]), indices)

plt.show()


# In[ ]:




