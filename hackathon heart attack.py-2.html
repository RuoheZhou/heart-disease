#!/usr/bin/env python
# coding: utf-8

# In[2]:


import pandas as pd
import numpy as np
from sklearn import preprocessing
import matplotlib.pyplot as plt 

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
import seaborn as sns


# In[187]:


heart1 = pd.read_csv("/Users/ruohezhou/Documents/hackathon/heart.csv")
heart2 = pd.read_csv("/Users/ruohezhou/Downloads/heart2.csv")
heart2.head()
heart1.shape


# In[188]:


heart2.head()


# In[189]:


del heart2['ca']
del heart2['thal']


# In[193]:


heart2 = heart2[['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs','restecg', 'thalach', 'exang','oldpeak','slope', 'target']]


# In[198]:


heart2.columns = ['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol', 'FastingBS',
       'RestingECG', 'MaxHR', 'ExerciseAngina', 'Oldpeak', 'ST_Slope',
       'HeartDisease']


# In[200]:


heart2.head()


# In[201]:


le = preprocessing.LabelEncoder()
for i in heart.columns:
    encode = le.fit_transform(heart[i])
    heart[i] = encode
heart.head()


# In[212]:


heart[['Oldpeak']] = heart[['Oldpeak']] / 10


# In[217]:


heart= heart.append(heart2)


# In[267]:


heart.to_csv('/Users/ruohezhou/Documents/shiny-prac\heart_combined.csv')


# In[263]:


x = np.array(heart[heart.columns[:11]])
y = np.array(heart.loc[:, 'HeartDisease'])


# In[264]:


from sklearn.model_selection import train_test_split

X_temp, X_test, y_temp, y_test =         train_test_split(x, y, test_size=0.3, 
                         shuffle=True, random_state=1, stratify=y)

X_train, X_valid, y_train, y_valid =         train_test_split(X_temp, y_temp, test_size=0.3,
                         shuffle=True, random_state=1, stratify=y_temp)


# In[265]:


#standardization
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler().fit(X_train)

X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

print('Train size', X_train.shape, 'class proportions', np.bincount(y_train))
print('Valid size', X_valid.shape, 'class proportions', np.bincount(y_valid))
print('Test size', X_test.shape, 'class proportions', np.bincount(y_test))


# train: 50%
# valid: 20%
# test: 30%

# ##Decision Tree

# In[222]:


from sklearn.tree import DecisionTreeClassifier


tree = DecisionTreeClassifier(random_state=1, min_samples_leaf=15)
tree.fit(X_train, y_train)

train_acc = tree.score(X_train, y_train)
valid_acc = tree.score(X_valid, y_valid)
test_acc = tree.score(X_test, y_test)

print(f'Training accuracy: {train_acc*100:.2f}%')
print(f'Validation accuracy: {valid_acc*100:.2f}%')
print(f'Depth: {tree.get_depth()}')


# In[223]:


heart.columns


# In[224]:


get_ipython().run_line_magic('matplotlib', 'inline')
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree


plt.figure(figsize=(5, 7))

plot_tree(tree, 
          filled=True, 
          rounded=True,
          feature_names=['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol', 'FastingBS',
       'RestingECG', 'MaxHR', 'ExerciseAngina', 'Oldpeak', 'ST_Slope'],
          class_names=['no heart disease', 
                       'heart disease']
         )

#plt.tight_layout()
plt.show()


# In[225]:


corrMatrix = heart.corr()
import seaborn as sn
import matplotlib.pyplot as plt
plt.figure(figsize=(16,5))
sn.heatmap(corrMatrix, annot=True)
plt.show()


# In[226]:


all_train_acc = []
all_valid_acc = []
all_hyperparam = []


for i in range(1, 40):
    tree = DecisionTreeClassifier(min_samples_leaf=i, random_state=1)
    tree.fit(X_train, y_train)
    train_acc = tree.score(X_train, y_train)
    valid_acc = tree.score(X_valid, y_valid)
    
    all_train_acc.append(train_acc*100)
    all_valid_acc.append(valid_acc*100)
    all_hyperparam.append(i)
    
plt.plot(all_hyperparam, all_train_acc, label='Training accuracy')
plt.plot(all_hyperparam, all_valid_acc, ls='--', label='Validation accuracy')
plt.xlabel('min_samples_leaf')
plt.ylabel('Accuracy in %')
plt.legend()
plt.show()


# In[227]:


tree.predict(X_test)


# KNN classifier

# In[228]:


from sklearn.neighbors import KNeighborsClassifier
clf = KNeighborsClassifier().fit(X_train, y_train)
clf.score(X_valid, y_valid)

clf.score(X_train, y_train)


# In[229]:


clf.score(X_test, y_test)


# In[230]:


clf.predict(X_test)


# In[231]:


from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression()

rfe = RFE(logreg, 12)

rfe = rfe.fit(X_train, y_train)


# In[ ]:





# In[232]:


rfe.score(X_test, y_test)


# In[233]:


rfe.score(X_valid, y_valid)


# In[234]:


from sklearn.model_selection import GridSearchCV
from mlxtend.evaluate import PredefinedHoldoutSplit
from sklearn.pipeline import make_pipeline


train_ind, valid_ind = train_test_split(np.arange(X_train.shape[0]),
                                        test_size=0.2, shuffle=True,
                                        random_state=0, stratify=y_train)

pipe = make_pipeline(StandardScaler(),
                     KNeighborsClassifier())

params = {'kneighborsclassifier__n_neighbors': [1, 3, 5, 7],
          'kneighborsclassifier__p': [1, 2]}

split = PredefinedHoldoutSplit(valid_indices=valid_ind)

grid = GridSearchCV(pipe,
                    param_grid=params,
                    cv=split)

grid.fit(X_train, y_train)


# In[235]:


grid.cv_results_


# In[236]:


for i,j in zip(grid.cv_results_['params'], grid.cv_results_['mean_test_score']):
    print(i, j)


# In[237]:


print(grid.best_score_)
print(grid.best_params_)


# In[238]:


clf = grid.best_estimator_

print('Test accuracy: %.2f%%' % (clf.score(X_test, y_test)*100))


# In[239]:


from keras.models import Sequential
from keras.layers import Dense
from keras.callbacks import EarlyStopping


# In[249]:


model = Sequential()
model.add(Dense(16, input_shape=(x.shape[1],), activation='relu')) 
model.add(Dense(16, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.summary() 


model.compile(optimizer='Adam', 
              loss='binary_crossentropy',
              metrics=['accuracy'])

  
es = EarlyStopping(monitor='val_accuracy', 
                                   mode='max', 
                                   patience=10,
                                   restore_best_weights=True)


history = model.fit(X_train,
                    y_train,
                    callbacks=[es],
                    epochs=80, 
                    batch_size=10,
                    validation_split=0.2,
                    shuffle=True,
                    verbose=1)


# In[250]:


history_dict = history.history

loss_values = history_dict['loss'] 
val_loss_values = history_dict['val_loss'] 


epochs = range(1, len(loss_values) + 1) 

# plot
plt.plot(epochs, loss_values, label='Training loss')
plt.plot(epochs, val_loss_values, 'grey', label='Validation loss')
plt.title('Training loss & validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()


# In[251]:


acc = history.history['accuracy']
val_acc = history.history['val_accuracy']


epochs = range(1, len(acc) + 1)

plt.plot(epochs, acc, label='Training accuracy')

plt.plot(epochs, val_acc, 'grey', label='Validation accuracy')
plt.title('Training accuracy & validation accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

np.max(val_acc)


# In[252]:


from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report


model.predict(X_test) 
np.round(model.predict(X_test),0) 


preds = np.round(model.predict(X_test),0)


print(confusion_matrix(y_test, preds)) 


print(classification_report(y_test, preds))


# In[253]:


model.predict(X_test)


# In[254]:


np.round(model.predict(X_test),0)


# In[255]:


from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators=250,
                                random_state=0)

forest.fit(X_train, y_train)
forest.feature_importances_


# 0:'Age'
# 1:'Sex'
# 2:'ChestPainType'
# 3:'RestingBP'
# 4:'Cholesterol'
# 5:'FastingBS'
# 6:'RestingECG'
# 7:'MaxHR'
# 8:'ExerciseAngina'
# 9:'Oldpeak'
# 10:'ST_Slope'

# In[256]:


std = np.std([tree.feature_importances_ for tree in forest.estimators_],
             axis=0)
indices = np.argsort(forest.feature_importances_)[::-1]

# Plot the feature importances of the forest
plt.figure()
plt.title("feature importance(random forest)")
plt.bar(range(x.shape[1]), importance_vals[indices],
        yerr=std[indices], align="center")
plt.xticks(range(x.shape[1]), indices)

plt.show()


# In[ ]:




